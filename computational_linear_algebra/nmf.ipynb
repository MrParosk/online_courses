{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF\n",
    "\n",
    "The idea is to decompose our matrix into two non-negative matricies, $W$ and $H$:\n",
    "\n",
    "$X \\approx W H$\n",
    "\n",
    "Note that non-negative matrix decomposition is not exact that the solutions are not unique. One of the reasons why NMF is popular is that positive factors are (sometimes) easier to interpret.\n",
    "\n",
    "We can find the two matricies by SGD. We try to minmize the different between $X$ and $W H$ and introduce an penalty when the elements are negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "X = tfidf.fit_transform(newsgroups.data) # (documents, vocab)\n",
    "X = X.todense()\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 100\n",
    "lambd = 10\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.abs(torch.normal(0, 0.01, size=(X.shape[0], num_components))).float().to(device)\n",
    "W.requires_grad = True\n",
    "H = torch.abs(torch.normal(0, 0.01, size=(num_components, X.shape[1]))).float().to(device)\n",
    "H.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalty(W, H):\n",
    "    return torch.clamp(-W, min=0).mean() + torch.clamp(-H, min=0).mean()\n",
    "\n",
    "def loss_fct(X, W, H):\n",
    "    return torch.norm(X - W @ H) + lambd * penalty(W, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([W, H], lr=1e-3, betas=(0.9, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 63.59439468383789\n",
      "Epoch: 100, Loss: 41.27014923095703\n",
      "Epoch: 200, Loss: 40.1865348815918\n",
      "Epoch: 300, Loss: 40.08699417114258\n",
      "Epoch: 400, Loss: 40.078739166259766\n",
      "Epoch: 500, Loss: 40.075218200683594\n",
      "Epoch: 600, Loss: 40.075225830078125\n",
      "Epoch: 700, Loss: 40.07392120361328\n",
      "Epoch: 800, Loss: 40.072601318359375\n",
      "Epoch: 900, Loss: 40.07277297973633\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fct(X, W, H)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0506,  0.0483,  0.0713,  ...,  0.0560,  0.0603,  0.0159],\n",
       "        [ 0.0327,  0.0294,  0.0477,  ...,  0.0410,  0.0361,  0.0117],\n",
       "        [ 0.0503,  0.0719,  0.0191,  ...,  0.0104,  0.0182,  0.0418],\n",
       "        ...,\n",
       "        [ 0.0441,  0.0494,  0.0585,  ...,  0.0342,  0.0145,  0.0278],\n",
       "        [-0.0007,  0.0592,  0.0430,  ...,  0.0129, -0.0108,  0.0674],\n",
       "        [ 0.0007,  0.0010,  0.0010,  ...,  0.0008,  0.0010,  0.0008]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
